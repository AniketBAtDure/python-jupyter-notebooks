{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPs with PyTorch\n",
    "This is my practice of the original by S. Raschka found [here](https://github.com/rasbt/deep-learning-book/blob/master/code/model_zoo/pytorch_ipynb/multilayer-perceptron.ipynb) except for the use of Fashion-MNIST and using `nn.Sequential` instead of subclassing `nn.Modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image batch shape =  torch.Size([64, 1, 28, 28])\n",
      "image label shape =  torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "############# Parameters #############\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Architecture\n",
    "num_features = 784\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "############# Fashion MNIST #############\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.FashionMNIST(root='data',\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(root='data',\n",
    "                                     train=False,\n",
    "                                     transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "\n",
    "# Check datasets\n",
    "for images, labels in train_loader:\n",
    "    print('image batch shape = ', images.shape),\n",
    "    print('image label shape = ', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "            \"\"\"This architecture is a densely connected 2 hidden layer network.\"\"\"\n",
    "            torch.nn.Linear(num_features, num_hidden_1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(num_hidden_2, num_classes)\n",
    "            )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "############# Define cost function and optimizer #############\n",
    "cost_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  001/010 | Batch:  000/937 | Cost:  0.5567\n",
      "Epoch:  001/010 | Batch:  100/937 | Cost:  0.4060\n",
      "Epoch:  001/010 | Batch:  200/937 | Cost:  0.7009\n",
      "Epoch:  001/010 | Batch:  300/937 | Cost:  0.4633\n",
      "Epoch:  001/010 | Batch:  400/937 | Cost:  0.3740\n",
      "Epoch:  001/010 | Batch:  500/937 | Cost:  0.4304\n",
      "Epoch:  001/010 | Batch:  600/937 | Cost:  0.4961\n",
      "Epoch:  001/010 | Batch:  700/937 | Cost:  0.5775\n",
      "Epoch:  001/010 | Batch:  800/937 | Cost:  0.4623\n",
      "Epoch:  001/010 | Batch:  900/937 | Cost:  0.6826\n",
      "Epoch:  001/010 training accuracy 81.35\n",
      "Epoch:  002/010 | Batch:  000/937 | Cost:  0.7363\n",
      "Epoch:  002/010 | Batch:  100/937 | Cost:  0.4921\n",
      "Epoch:  002/010 | Batch:  200/937 | Cost:  0.4393\n",
      "Epoch:  002/010 | Batch:  300/937 | Cost:  0.4756\n",
      "Epoch:  002/010 | Batch:  400/937 | Cost:  0.5830\n",
      "Epoch:  002/010 | Batch:  500/937 | Cost:  0.5604\n",
      "Epoch:  002/010 | Batch:  600/937 | Cost:  0.4138\n",
      "Epoch:  002/010 | Batch:  700/937 | Cost:  0.4318\n",
      "Epoch:  002/010 | Batch:  800/937 | Cost:  0.3804\n",
      "Epoch:  002/010 | Batch:  900/937 | Cost:  0.6541\n",
      "Epoch:  002/010 training accuracy 82.52\n",
      "Epoch:  003/010 | Batch:  000/937 | Cost:  0.5762\n",
      "Epoch:  003/010 | Batch:  100/937 | Cost:  0.4029\n",
      "Epoch:  003/010 | Batch:  200/937 | Cost:  0.5816\n",
      "Epoch:  003/010 | Batch:  300/937 | Cost:  0.4135\n",
      "Epoch:  003/010 | Batch:  400/937 | Cost:  0.5920\n",
      "Epoch:  003/010 | Batch:  500/937 | Cost:  0.7270\n",
      "Epoch:  003/010 | Batch:  600/937 | Cost:  0.3995\n",
      "Epoch:  003/010 | Batch:  700/937 | Cost:  0.3823\n",
      "Epoch:  003/010 | Batch:  800/937 | Cost:  0.4450\n",
      "Epoch:  003/010 | Batch:  900/937 | Cost:  0.4741\n",
      "Epoch:  003/010 training accuracy 81.34\n",
      "Epoch:  004/010 | Batch:  000/937 | Cost:  0.3365\n",
      "Epoch:  004/010 | Batch:  100/937 | Cost:  0.5269\n",
      "Epoch:  004/010 | Batch:  200/937 | Cost:  0.5372\n",
      "Epoch:  004/010 | Batch:  300/937 | Cost:  0.4681\n",
      "Epoch:  004/010 | Batch:  400/937 | Cost:  0.5937\n",
      "Epoch:  004/010 | Batch:  500/937 | Cost:  0.4109\n",
      "Epoch:  004/010 | Batch:  600/937 | Cost:  0.4091\n",
      "Epoch:  004/010 | Batch:  700/937 | Cost:  0.4267\n",
      "Epoch:  004/010 | Batch:  800/937 | Cost:  0.2575\n",
      "Epoch:  004/010 | Batch:  900/937 | Cost:  0.6468\n",
      "Epoch:  004/010 training accuracy 83.83\n",
      "Epoch:  005/010 | Batch:  000/937 | Cost:  0.4662\n",
      "Epoch:  005/010 | Batch:  100/937 | Cost:  0.4755\n",
      "Epoch:  005/010 | Batch:  200/937 | Cost:  0.3346\n",
      "Epoch:  005/010 | Batch:  300/937 | Cost:  0.4247\n",
      "Epoch:  005/010 | Batch:  400/937 | Cost:  0.3385\n",
      "Epoch:  005/010 | Batch:  500/937 | Cost:  0.4485\n",
      "Epoch:  005/010 | Batch:  600/937 | Cost:  0.3808\n",
      "Epoch:  005/010 | Batch:  700/937 | Cost:  0.4676\n",
      "Epoch:  005/010 | Batch:  800/937 | Cost:  0.4396\n",
      "Epoch:  005/010 | Batch:  900/937 | Cost:  0.3931\n",
      "Epoch:  005/010 training accuracy 81.58\n",
      "Epoch:  006/010 | Batch:  000/937 | Cost:  0.4061\n",
      "Epoch:  006/010 | Batch:  100/937 | Cost:  0.6326\n",
      "Epoch:  006/010 | Batch:  200/937 | Cost:  0.5673\n",
      "Epoch:  006/010 | Batch:  300/937 | Cost:  0.3146\n",
      "Epoch:  006/010 | Batch:  400/937 | Cost:  0.3310\n",
      "Epoch:  006/010 | Batch:  500/937 | Cost:  0.4976\n",
      "Epoch:  006/010 | Batch:  600/937 | Cost:  0.3823\n",
      "Epoch:  006/010 | Batch:  700/937 | Cost:  0.4208\n",
      "Epoch:  006/010 | Batch:  800/937 | Cost:  0.4248\n",
      "Epoch:  006/010 | Batch:  900/937 | Cost:  0.7198\n",
      "Epoch:  006/010 training accuracy 84.74\n",
      "Epoch:  007/010 | Batch:  000/937 | Cost:  0.5067\n",
      "Epoch:  007/010 | Batch:  100/937 | Cost:  0.5938\n",
      "Epoch:  007/010 | Batch:  200/937 | Cost:  0.2869\n",
      "Epoch:  007/010 | Batch:  300/937 | Cost:  0.3096\n",
      "Epoch:  007/010 | Batch:  400/937 | Cost:  0.2665\n",
      "Epoch:  007/010 | Batch:  500/937 | Cost:  0.5094\n",
      "Epoch:  007/010 | Batch:  600/937 | Cost:  0.4507\n",
      "Epoch:  007/010 | Batch:  700/937 | Cost:  0.3364\n",
      "Epoch:  007/010 | Batch:  800/937 | Cost:  0.4091\n",
      "Epoch:  007/010 | Batch:  900/937 | Cost:  0.3704\n",
      "Epoch:  007/010 training accuracy 84.30\n",
      "Epoch:  008/010 | Batch:  000/937 | Cost:  0.4296\n",
      "Epoch:  008/010 | Batch:  100/937 | Cost:  0.6715\n",
      "Epoch:  008/010 | Batch:  200/937 | Cost:  0.5632\n",
      "Epoch:  008/010 | Batch:  300/937 | Cost:  0.4350\n",
      "Epoch:  008/010 | Batch:  400/937 | Cost:  0.6145\n",
      "Epoch:  008/010 | Batch:  500/937 | Cost:  0.3033\n",
      "Epoch:  008/010 | Batch:  600/937 | Cost:  0.3907\n",
      "Epoch:  008/010 | Batch:  700/937 | Cost:  0.4165\n",
      "Epoch:  008/010 | Batch:  800/937 | Cost:  0.6448\n",
      "Epoch:  008/010 | Batch:  900/937 | Cost:  0.2495\n",
      "Epoch:  008/010 training accuracy 84.96\n",
      "Epoch:  009/010 | Batch:  000/937 | Cost:  0.5013\n",
      "Epoch:  009/010 | Batch:  100/937 | Cost:  0.4081\n",
      "Epoch:  009/010 | Batch:  200/937 | Cost:  0.3088\n",
      "Epoch:  009/010 | Batch:  300/937 | Cost:  0.3234\n",
      "Epoch:  009/010 | Batch:  400/937 | Cost:  0.3099\n",
      "Epoch:  009/010 | Batch:  500/937 | Cost:  0.5318\n",
      "Epoch:  009/010 | Batch:  600/937 | Cost:  0.3479\n",
      "Epoch:  009/010 | Batch:  700/937 | Cost:  0.4059\n",
      "Epoch:  009/010 | Batch:  800/937 | Cost:  0.3749\n",
      "Epoch:  009/010 | Batch:  900/937 | Cost:  0.2546\n",
      "Epoch:  009/010 training accuracy 85.82\n",
      "Epoch:  010/010 | Batch:  000/937 | Cost:  0.3412\n",
      "Epoch:  010/010 | Batch:  100/937 | Cost:  0.2776\n",
      "Epoch:  010/010 | Batch:  200/937 | Cost:  0.4153\n",
      "Epoch:  010/010 | Batch:  300/937 | Cost:  0.4111\n",
      "Epoch:  010/010 | Batch:  400/937 | Cost:  0.4410\n",
      "Epoch:  010/010 | Batch:  500/937 | Cost:  0.3881\n",
      "Epoch:  010/010 | Batch:  600/937 | Cost:  0.4396\n",
      "Epoch:  010/010 | Batch:  700/937 | Cost:  0.4177\n",
      "Epoch:  010/010 | Batch:  800/937 | Cost:  0.5288\n",
      "Epoch:  010/010 | Batch:  900/937 | Cost:  0.4540\n",
      "Epoch:  010/010 training accuracy 85.99\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for features, targets in data_loader:\n",
    "        features = Variable(features.view(-1, 28*28))\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "\n",
    "        logits = model(features)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "\n",
    "        _, predicted_labels = torch.max(probas.data, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels.cpu() == targets).sum()\n",
    "    return correct_pred / num_examples * 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \"\"\"Train\"\"\"\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = Variable(features.view(-1, 28*28))\n",
    "        targets = Variable(targets)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            features, targets = features.cuda(), targets.cuda()\n",
    "            \n",
    "        ### Forward\n",
    "        logits = model(features)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        cost = cost_fn(logits, targets)\n",
    "        # Sets gradients of all model parameters to zero:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backprop\n",
    "        cost.backward()\n",
    "        \n",
    "        ### Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### Logging\n",
    "        if not batch_idx % 100:\n",
    "            print('Epoch:  %03d/%03d | Batch:  %03d/%03d | Cost:  %.4f' %\n",
    "                 (epoch+1, num_epochs, batch_idx,\n",
    "                 len(train_dataset)//batch_size, cost.data[0]))\n",
    "            \n",
    "    print('Epoch:  %03d/%03d training accuracy %.2f' %\n",
    "         (epoch+1, num_epochs,\n",
    "         compute_accuracy(model, train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 84.46%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
